\setlength{\parindent}{10ex}

\section{Future Challenges for Parallel Computing and HPC}

\subsection{Scalable Energy Consumption}
Scalable Energy is a serious problem for the future of HPC.
Processors are slowly approaching a point where it is impossible to fit more transistors on a piece of silicon.
This limits the future efficiency gains that processors can receive. 
This will cause the energy scaling on a HPC cluster to be linear, and potentially unsustainable.
This is unless, we create new and plentiful energy supplies, or innovate energy efficiency on processors.

\par
Creating clusters of energy efficient computers is a simple solution to this problem.
This solution hinges on the ability on manufactures to create faster CPUs that use less power.
Advancements in carbon nano tubule technology will help beyond the current limitations with silicon, but even that approach has its limits.
Eventually, classical computing will meet a bottleneck that will limit the energy scaling of clusters.
This will make it impractical to scale classical clusters of computers large enough to meet our computing needs,

\subsection{Big Data Processing}
Big Data processing is a prominent use case for HPC and Parallel Computing. 
However, Big data is a ever increasing problem.
As the problems increase, so do the demands for infrastructure.
Managing distributed memory, storage, and interconnection in the cluster becomes an increasingly difficult problem.

\par
Machine Learning on big datasets has yielded excellent models.
The problem with this approach is normalizing all of the data.
For a problem to be paralleled the data must be regular.
Big Data is not always regular.
Data science will need to innovate to solve these big data problems.

\par 
Storage for Big Data is also a concern.
Unimaginable amounts of data will be harvested from the internet in the future.
Storing the zetabytes of future information and making it available to parallel algorithms for processing is a very real and challenging problem.
There are distributed file system solutions now that could scale to meet these challenges.
Ideally, a better solution will be innovated to solve these storage problems.
In the mean time, we will keep daisy chaining hard drives!

\subsection{Trends in Innovation}
Computing innovation in the 20th century was driven by the needs of the research and scientific communities.
This directly led to innovation in parallel architectures for these communities.
The 21st century is changing the paradigm on computing innovation.
Consumer electronics have been driving innovation.
This leads to the needs of that domain taking precedence.
There is a need for new algorithms to overcome new problems.
However, the new source of innovation threatens to neglect innovation in the HPC domain.

\par
I argue that innovation for consumer electronics does not align with desired innovation for HPC.
New parallel algorithms for light ray tracing in graphics may not translate to the HPC domain.
This is purely a supply and demand problem. 
The demand for consumer electronic innovations is very high.
Therefore, the supply will be funded.
The demand for HPC innovations is low.
Therefore, the supply will receive less funding.

\par 
This shift in the focus of computing innovation is a true threat to HPC and Parallel computing advancements.
Many of this domains biggest problems will require innovative solutions to fix.
Ideally, innovations in other domains will be applicable to the HPC domain.
Things like energy efficient processors, storage gains, and GPU algorithms could make the leap.
In general, it is unrealistic to hope that the gaming market will drive innovation in the HPC domain. 
Maybe one day the focus of innovation will pivot back toward the needs of HPC and parallel computing.
